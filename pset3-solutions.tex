\documentclass[11pt]{article}

% Packages for math. MUST LOAD BEFORE FONT PACKAGES
\usepackage{amsmath, amssymb, amsthm}


% Set font to Palatino
\usepackage{newpxmath}
\usepackage{newpxtext}
\linespread{1.05}         % Palatino needs more leading (space between lines)
\usepackage[T1]{fontenc}

\usepackage{microtype}
\usepackage{url}
\usepackage{tikz}
\usetikzlibrary{patterns}
\usepackage{enumitem}

\usepackage{color}
\usepackage{mathtools}
\usepackage{tcolorbox}



\usepackage[colorlinks=true,urlcolor=blue,linkcolor=blue]{hyperref}


% Page margins
\usepackage[margin=1in]{geometry}


% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\theoremstyle{definition}
\newtheorem{problem}[theorem]{Problem}
\newtheorem*{question}{Question}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem*{lemma*}{Lemma}

% Delimiter macros
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\brac}[1]{\left[ #1 \right]}
\newcommand{\iprod}[1]{\langle #1 \rangle}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\norm}[1]{\left\| #1 \right\|}

% Macros for real and natural numbers
\newcommand{\R}{\mathbb{R}} % Real numbers
\newcommand{\N}{\mathbb{N}} % Natural numbers

\renewcommand{\epsilon}{\varepsilon}
\newcommand{\eps}{\epsilon}

% Alphabet
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\wt}[1]{\widetilde{#1}}


% Macros for probability
\DeclareMathOperator{\E}{\mathbb{E}} % Expectation
\DeclareMathOperator{\pE}{\widetilde{\mathbb{E}}} % Pseudoexpectation

% Macros for optimization
\newcommand{\OPT}{\text{OPT}}
\newcommand{\SoS}{\text{SoS}}

% Macro for indicator function
\newcommand{\Ind}[1]{\mathbf{1}\left ( #1\right )}
\newcommand{\Id}{\mathrm{Id}}
\newcommand{\indic}{\mathbf{1}}
\newcommand{\opnorm}[1]{\left\|#1\right\|_{\mathrm{op}}}
\newcommand{\Bin}{\operatorname{Bin}}
\newcommand{\sgn}{\operatorname{sign}}

% Title and author information
\title{Problem Set 3 Solutions}
\author{Samuel B. Hopkins}
\date{\today}

% Document starts here
\begin{document}

\maketitle

\begin{problem}[SoS proof for clique size bound]
  Let $G$ be a graph drawn from $G(n,1/2)$. Show that with high probability, there exists a sum-of-squares proof of constant degree that certifies that $G$ does not contain a clique of size greater than $O(\sqrt{n \log n})$.
\end{problem}

\paragraph{Solution}

  Given a graph $G$, consider the system of polynomials
  \[ \mathcal{P} = \left\{ \begin{matrix*}[l] x_i^2 = x_i & \forall i \in [n], \\ x_i x_j = 0 & \forall ij \not\in E(G) \end{matrix*} \right\}. \]
  The first collection of constraints indicates that the $x_i$ behave like boolean variables, while the second collection of constraints implies that the set $\{i : x_i = 1\}$ forms a clique. Given a solution to the above system, the size of the corresponding clique is $\sum_{i \in [n]} x_i$.

  We shall show that with high probability over $G \sim G(n,1/2)$, $\mathcal{P} \vdash_4 \sum x_i \le O(\sqrt{n \log n})$. Let $A$ be the adjacency matrix of $G$, and $J$ the all-ones matrix.

  Let $\pE$ be an arbitrary pseudoexpectation satisfying $\mathcal{P}$. For starters, we have
  \[ \left(\pE \sum x_i\right)^2 \le \pE \left( \sum x_i \right)^2 = \pE x^\top J x = \pE \sum_{i,j \in [n]} x_i x_j = 2 \cdot \pE \sum_{ij \in E} x_i x_j = 2 \cdot \pE x^\top A x. \]
  Here, the first inequality is Cauchy-Schwarz, and the second-to-last equality is because $\pE x_i x_j = 0$ for any non-edge $ij$. Now, we have
  \[
    \pE x^\top A x = \pE x^\top \left( \frac{1}{2} J \right) x + \pE x^\top \left( A - \frac{1}{2} J \right) x = \frac{1}{2} \pE x^\top A x + \pE x^\top \left( A - \frac{1}{2} J \right) x,
  \]
  so
  \begin{equation}
    \label{q1:main}
    \left(\pE \sum x_i\right)^2 \lesssim \pE x^\top A x \lesssim \pE x^\top \left( A - \frac{1}{2} J \right) x.
  \end{equation}
  To conclude, we shall show that with high probability, $\opnorm{ A - \frac{1}{2} J } = O(\sqrt{n \log n})$. Given this, we are done: as we saw early in the course, this would imply that $\pE x^\top \left( A - \frac{1}{2} J \right) x \le \pE \opnorm{ A - \frac{1}{2} J } \sum x_i^2 = O(\sqrt{n \log n}) \cdot \pE \sum x_i$, and plugging this back into \eqref{q1:main} completes the proof.

  It remains to prove the high-probability bound on the operator norm. Let $B = A - \frac{1}{2} J + \frac{1}{2} \Id$. For each pair of distinct indices $i,j \in [n]$, let $B^{(ij)}$ be the matrix such that $B^{(ij)}_{ij} = B^{(ij)}_{ji}$ are uniformly randomly drawn from $\{\pm 1\}$, and all other entries are $0$. Note that $B$ has the same distribution as $\sum_{i,j \in [n] \text{ distinct}} B^{(ij)}$. Clearly, the operator norm of any $B^{(ij)}$ is almost surely bounded by $2$. It is also not difficult to check that for any $i,j$, $\E (B^{(ij)})^2 = e_i e_i^\top + e_j e_j^\top$ and so,
  \[ \opnorm{\E \sum_{i,j \in [n] \text{ distinct}} (B^{(ij)})^2} = \opnorm{ (n-1) \Id } \le n. \]
  The matrix Bernstein inequality implies that
  \[ \E \opnorm{B} \le O\left( \sqrt{ \opnorm{ \E \sum_{i,j \in [n] \text{ distinct}} (B^{(ij)})^2 } } \cdot \sqrt{\log n} + 2 \log n \right) = O(\sqrt{n \log n}). \]
  Markov's inequality implies that with high probability (say $0.99$), $\opnorm{B} \le O(\sqrt{n \log n})$, so
  \[ \opnorm{ A - \frac{1}{2} J } = \le \opnorm{B} + \opnorm{ \frac{1}{2} \Id } = O(\sqrt{n \log n}), \]
  completing the proof.

\clearpage

\begin{problem}[Robustness to adversarial modification]
  Suppose a malicious adversary is allowed to modify any subset of $n^{0.99}$ edges of a graph drawn from $G(n,1/2)$. Show that, despite this, there exists with high probability a constant-degree SoS proof that certifies the graph does not contain any clique of size greater than $O(\sqrt{n \log n})$.
\end{problem}

\paragraph{Solution}

  We shall use essentially the same proof as in the first question. Let $G$ be the true graph drawn from $G(n,1/2)$ with adjacency matrix $A$, and $\wt{G}$ the corrupted graph observed by the algorithm $\wt{A}$. Then, we have
  \[ \opnorm{\wt{A} - \frac{1}{2} J} \le \opnorm{ A - \frac{1}{2} J} + \opnorm{\wt{A} - A}. \]
  However, because the adversary can modify only $n^{0.99}$ edges (that is, change $n^{0.99}$ entries of $A$ from $0$ to $1$ or vice-versa),
  \[ \opnorm{\wt{A} - A} \le \norm{\wt{A} - A}_F \le n^{0.99/2} = o(\sqrt{n}). \]
  Thus, with high probability, $\opnorm{ A - \frac{1}{2} J } \le O(\sqrt{n \log n})$, and by the above argument, $\opnorm{\wt{A} - \frac{1}{2} J} \le O(\sqrt{n \log n})$, completing the proof by the same argument as in the first question.

\clearpage

\begin{problem}[Planted $2$-XOR]
  Let $\phi$ be a random instance of $2$-XOR over $\{\pm 1\}$, sampled in the following way. First, choose $x^* \in \{\pm 1\}^n$. Then, for each $(i,j) \in [n]^2$, with probability $\frac{C n \log n}{n^2}$,
  \begin{enumerate}
    \item with probability $0.99$, add the constraint $x_i x_j = x_i^* x_j^*$ to $\phi$, and
    \item otherwise, add the constraint $x_i x_j = - x_i^* x_j^*$ to $\phi$.
  \end{enumerate}
  The resulting instance $\phi$ should have about $C n \log n$ equations.
  \begin{enumerate}[label=(\alph*)]
    \item Show that for sufficiently large (constant) $C$, with high probability, there exists $y \in \{\pm 1\}^n$ which satisfies a $0.98$ fraction of the equations in $\phi$.
    \item Show that for sufficiently large (constant) $C$, there is a polynomial-time algorithm which finds some $y \in \{\pm 1\}^n$ which satisfies at least a $0.97$ fraction of the equations in $\phi$.
  \end{enumerate}
\end{problem}

\paragraph{Solution}

  \begin{enumerate}[label=(\alph*)]
    \item We shall show that with high probability, $x^*$ satisfies a $0.98$ fraction of the equations in $\phi$. First off, we may use the Chernoff bound to show that with high probability, there are about $C n \log n (1-o(1))$ clauses. Indeed, the number of clauses is distributed as the binomial random variable $\Bin(n^2 , \frac{C n \log n}{n^2})$. Then,
    \[ \Pr \left[ \left|\text{\# clauses} - C n \log n \right| \ge n \sqrt{\log n} \right] \le \exp\left( - O\left(\frac{(n \sqrt{\log n})^2}{n^2}\right) \right) = o(1). \]
    Now, suppose we have conditioned on there being $m = (C n \log n)(1-o(1))$ clauses. Then, the number of clauses satisfied by $x^*$ is distributed as $\Bin(m , 0.99)$. Again, a Chernoff bound implies that
    \[ \Pr\left[ \text{\# clauses satisfied by $x^*$} \le 0.98m \right] \le \exp\left( - O \left( \frac{(0.01m)^2}{m} \right) \right) = o(1). \]
    Putting these two together, we get that
    \[ \Pr\left[ \text{\# clauses} = Cn\log n(1+o(1)) \text{ and } \text{fraction of clauses satisfied by $x^*$} \ge 0.98 \right] = o(1), \]
    completing the proof. Note that the $0.98$ here can be replaced with a constant arbitrarily close to $0.99$. We will use this in the second part.

    \item For this entire part, let $\eps > 0$ be a small constant---we shall set it to be sufficiently small in the end so that an objective value of at least $0.97$ is attained.

    Suppose that the set of constraints is $\{x_i x_j = A_{ij} : ij \in E\}$, where each $A_{ij} \in \{\pm 1\}$ and $E$ is the set of pairs involved in constraints. Let $A$ be the corresponding constraint matrix, whose $ij$th entry is $A_{ij}$ if $ij \in E$, and is $0$ otherwise.This $2$-XOR instance may naturally be represented as an optimization problem, where the goal is to maximize $\frac{1}{m} x^\top A x$ subject to the constraints $x_i^2 = 1$ for all $i$.

    Consider the natural degree-$4$ sum-of-squares relaxation of the above optimization problem, and suppose it returns a pseudoexpectation $\pE$ over random variables $x_i$, with $\pE \vDash x_i^2 = 1$ for all $i$, and $\pE \frac{1}{m} x^\top A x \ge (0.98-\eps)$. We know that such a pseudoexpectation exists with high probability by the strengthened version of (a)---note that we have a $0.98-\eps$ instead of a $0.99-\eps$ here because $x^\top A x$ is equal to $(\# \text{satisfied clauses}) - (\# \text{unsatisfied clauses})$.

    To start, let us show that the objective value attained by a vector essentially only depends on its correlation with $x^*$. For any $y \in \{\pm 1\}^n$, we have
    \[ y^\top A y = \frac{0.98 C \log n}{n} \langle x^*,y\rangle^2 + \left\langle yy^\top , A - 0.98 \cdot \frac{C \log n}{n} (x^*)(x^*)^\top \right\rangle. \]
    Motivated by the fact that $\E[A \mid x^*] = 0.98 \cdot \frac{C \log n}{n} \cdot (x^*)(x^*)^\top$, we claim the following.

    \textbf{Lemma.} With high probability, $\opnorm{ A - 0.98 \cdot \frac{C \log n}{n} (x^*)(x^*)^\top } \le \eps C\log n$ for sufficiently large constant $C$.
    
    \emph{Proof.} The above operator norm bound essentially follows from the matrix Bernstein inequality. Indeed, let $B$ be this matrix, and set $B^{(ij)}$ to be the symmetric random matrix such that
    \[ B^{(ij)}_{ij} = B^{(ij)}_{ji} = \begin{cases} - 0.98 \cdot \frac{C \log n}{n} \cdot x_i^* x_j^*, & \text{w.p. } 1 - \frac{C \log n}{n}, \\ \left( 1 - 0.98 \frac{C \log n}{n} \right) x_i^* x_j^*, & \text{w.p. } 0.99 \cdot \frac{C \log n}{n}, \\ - \left( 1 + 0.98 \frac{C \log n}{n} \right) x_i^* x_j^*, & \text{w.p. } 0.01 \cdot \frac{C \log n}{n} \end{cases} \]
    and all other entries are equal to $0$. Clearly, the operator norm of $B^{(ij)}$ is almost surely bounded by $2$, and
    \begin{align*}
      \E (B^{(ij)})^2 &= (e_{ii} + e_{jj}) \cdot \left( 1 - \frac{C \log n}{n} \right) \cdot \left( 0.98 \frac{C \log n}{n} \right)^2 + \left( 0.99 \cdot \frac{C \log n}{n} \right) \cdot \left( 1 - 0.98 \frac{C \log n}{n} \right)^2 \\
      &\qquad\qquad\qquad\qquad\qquad\qquad+ \left( 0.01 \cdot \frac{C \log n}{n} \right) \cdot \left( 1 + 0.98 \frac{C \log n}{n} \right)^2 \\
        &\le (e_{ii} + e_{jj}) \frac{C \log n}{n} \left( 1 + o(1) \right).
    \end{align*}
    It follows that
    \[ \opnorm{ \sum_{i,j} (B^{(ij)})^2 } \le O(C \log n). \]
    The matrix Bernstein inequality implies that
    \[ \E \opnorm{ A - 0.98 \cdot \frac{C \log n}{n} (x^*)(x^*)^\top } \le O\left( \sqrt{C} \log n \right). \]
    This implies that for sufficiently large constant $C$ (of the order $\Omega\left(\frac{1}{\eps^2}\right)$), with high probability, $\opnorm{ A - 0.99 \cdot \frac{C \log n}{n} (x^*)(x^*)^\top } \le \eps C \log n$. Since $m = C n \log n (1+o(1))$, we can plug this back into \eqref{eq:q3-high-corr} to conclude that
    \begin{equation}
      \label{q3:eq-main}
      y^\top A y \ge \frac{0.98 C \log n}{n} \langle x^*,y\rangle^2 - \eps m.
    \end{equation}
    Further note that the above inequality is true (with high probability) in a sum-of-squares manner, since all we used was a bound on the operator norm of a matrix. \qed\qedhere

    The algorithm we will use is as follows. We pick a uniformly random row $y$ of $\pE xx^\top$, and then round it to a vector $\sgn(y)$ on the hypercube as $\sgn(y)_i = \sgn(y_i)$.

    To analyze this, we must show that
    \begin{enumerate}[label=(\roman*)]
      \item a random row $y$ of $\pE xx^\top$ is well-correlated with $x^*$, and
      \item rounding $y$ to a vector on the hypercube does not lose too much in the objective value.
    \end{enumerate}

    Let us begin with (ii). Let $y$ be a vector of norm $\sqrt{n}$ such that $\langle y,x^*\rangle \ge (1-\eps) n$. Then,
    \[ \|y-x^*\|^2 \le 2\eps n, \]
    so there are at most $2\eps n$ indices $i$ such that $|y_i - x_i^*| \ge 1$. In particular, this implies that there are most $2\eps n$ indices $i$ such that $\sgn(y_i) \ne \sgn(x_i^*)$, and thus $\langle \sgn(y) , x^* \rangle \ge (1-2\eps)n$, yielding the desideratum by \eqref{q3:eq-main}. 

    To conclude, we must show (i), that a random row of $\pE xx^\top$ is well-correlated with $x^*$. We may write
    \begin{equation}
      \label{eq:q3-high-corr}
      (0.98-\eps) m \le \pE x^\top A x = 0.98 \cdot \frac{C \log n}{n} \pE \langle x , x^* \rangle^2 + \left\langle \pE xx^\top , A - 0.98 \cdot \frac{C\log n}{n} (x^*)(x^*)^\top \right\rangle.
    \end{equation}
    By the operator norm bound from earlier, and the concentration of the number of clauses, we have
    \[ (0.98 - \eps) m \le 0.98 \cdot \frac{C \log n}{n} \pE \langle x,x^*\rangle^2 + \eps m. \]
    Because $m = C n \log n (1+o(1))$,
    \[ \left\langle \pE xx^T , (x^*)(x^*)^\top \right\rangle \ge (1-O(\eps))n^2. \]
    For a (uniformly) random row $v_i$ of $\pE xx^\top$, we have
    \begin{align*}
      \E |\langle v_i , x^*\rangle| &= \frac{1}{n} \sum_{i \in [n]} \left| \sum_{j \in [n]} (v_i)_j x^*_j \right| \\
        &= \frac{1}{n} \sum_{i \in [n]} \left| \sum_{j \in [n]} \pE [x_i x_j] \cdot x^*_i x^*_j \right| \\
        &\ge \frac{1}{n} \left\langle \pE xx^\top , (x^*)(x^*)^\top \right\rangle \ge (1-O(\eps))n.
    \end{align*}
    Because we almost surely have $\norm{v_i}^2 = \sum_{j \in [n]} \left(\pE x_ix_j\right)^2 \le \sum_{j \in [n]} \pE x_i^2 x_j^2 \le n$, it follows that with high probability, a uniformly random row of $\pE xx^\top$ is $(1-O(\eps))$-correlated with $x^*$, completing the proof by prior discussion.


  \end{enumerate}

\end{document}
