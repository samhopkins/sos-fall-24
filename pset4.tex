\documentclass[11pt]{article}

% Packages for math. MUST LOAD BEFORE FONT PACKAGES
\usepackage{amsmath, amssymb, amsthm}


% Set font to Palatino
\usepackage{newpxmath}
\usepackage{newpxtext}
\linespread{1.05}         % Palatino needs more leading (space between lines)
\usepackage[T1]{fontenc}

\usepackage{microtype}
\usepackage{url}
\usepackage{tikz}
\usetikzlibrary{patterns}

\usepackage{color}
\usepackage{tcolorbox}
\usepackage{enumitem}



\usepackage[colorlinks=true,urlcolor=blue,linkcolor=blue]{hyperref}


% Page margins
\usepackage[margin=1in]{geometry}


% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\theoremstyle{definition}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{bproblem}[theorem]{Bonus Problem}
\newtheorem*{question}{Question}
\newtheorem{exercise}[theorem]{Exercise}

% Delimiter macros
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\brac}[1]{\left[ #1 \right]}
\newcommand{\iprod}[1]{\langle #1 \rangle}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\norm}[1]{\left\| #1 \right\|}
\newcommand{\poly}{\mathrm{poly}}

% Macros for real and natural numbers
\newcommand{\R}{\mathbb{R}} % Real numbers
\newcommand{\N}{\mathbb{N}} % Natural numbers

\renewcommand{\epsilon}{\varepsilon}
\newcommand{\eps}{\epsilon}

% Alphabet
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\Id}{\mathrm{Id}}


% Macros for probability
\DeclareMathOperator{\E}{\mathbb{E}} % Expectation
\DeclareMathOperator{\pE}{\widetilde{\mathbb{E}}} % Pseudoexpectation
\newcommand{\Var}{\mathbf{Var}}

% Macros for optimization
\newcommand{\OPT}{\mathsf{OPT}}
\newcommand{\SoS}{\text{SoS}}

% Macro for indicator function
\newcommand{\Ind}[1]{\mathbf{1}\left ( #1\right )}
\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\ER}{\mathsf{G}}

% Title and author information
\title{Problem Set 4}
\author{Samuel B. Hopkins}
\date{Last updated \today}

% Document starts here
\begin{document}

\maketitle

Due: 12/3, 11:59pm.

Please typeset your solutions in LaTeX.

\begin{problem}[Sparse robust mean estimation]

  In this problem, we will solve a sparse version of robust mean estimation.  Let $\mu \in \R^d$ be an unknown $k$-sparse vector, in that only $k$ of its entries are non-zero.  First $n = \widetilde{\Omega}(k^2 (\log d)/\epsilon^2)$ samples $v_1, \dots, v_n \in \R^d$ are drawn from $\mathcal{N}(\mu, \Id)$. Then an adversary alters $\epsilon n$ of the samples and reorders them arbitrarily.  We observe the resulting dataset $v_1', \dots , v_n'$.   Our goal will be to give an algorithm for estimating $\mu$ from these samples.

  \begin{enumerate}[label=(\alph*)]
    \item Let $\overline{v} = \frac{1}{n} \sum_{i = 1}^n v_i$.  Prove that with $0.99$ probability, for all $k$-sparse vectors $u \in \R^d$ with $\| u \| = 1$,
    \[
    \langle u, \overline{v} - \mu \rangle^2 \leq \epsilon^2 \,.  
    \]
    \item Define $\Sigma = \frac{1}{n} \sum_{i = 1}^n (v_i - \overline{v}) (v_i - \overline{v})^T$. 
     Prove that with $0.99$ probability, $|\Sigma_{ij}| \leq 1/k$ for $i \neq j$ and $| \Sigma_{ii} - 1| \leq 1/k$ for all $i,j \in [d]$.
    \item Consider the following system, which we call $\mathcal{S}$, with scalar variables $w_1, \dots , w_n$ and $d$-dimensional variables $z, z_1, \dots ,  z_n$
    \[
    \begin{split}
    &w_i^2 = w_i \\
    & \sum_{i = 1}^n w_i \geq (1 - \epsilon) n \\
    &w_i(z_i - v_i') = 0 \\
    &\overline{z} =  \frac{1}{n} \sum_{i = 1}^n z_i \; , \; \Sigma = \frac{1}{n} \sum_{i = 1}^n (z_i - \overline{z}) (z_i - \overline{z})^T \\ 
    &-\frac{1}{k} \leq \Sigma_{ij} \leq \frac{1}{k}  \;\;\;\; \text{ for all } i \neq j  \\
    &-\frac{1}{k} \leq \Sigma_{ii} - 1 \leq \frac{1}{k} \;\;\;\; \text{ for all } i
    \end{split}
    \]
    Prove that with $0.99$ probability, there is a feasible solution to this system where the $w_i$ are indicators of the clean samples and the $z_i$ are the actual clean samples.
    \\\\
    From now on, assume that the events in (a), (b), (c) hold.
    \item Now we consider the SoS relaxation of the system $\mathcal{S}$.  Let $u \in \R^d$ be an arbitrary $k$-sparse vector with $\| u \| = 1$.  Prove that 
    \[
    \mathcal{S} \vdash_2 \sum_{i = 1}^n \langle u, z_i - v_i \rangle^2  \leq 10 n (1 + \langle u, \overline{z} - \mu \rangle^2  )
    \]
    where recall $v_i$ are the clean samples drawn from $N(\mu, I)$.    
    \item Let $u \in \R^d$ be an arbitrary $k$-sparse vector with $\| u \| = 1$.  Use part (c) to prove that 
    \[
    \mathcal{S} \vdash_4 \langle u, \overline{z} - \overline{v} \rangle^2 \leq   100 \epsilon  (1 + \langle u, \overline{z} - \mu \rangle^2  )
    \]
    \item Use part (e) to deduce that 
    \[
    \mathcal{S} \vdash_4 \langle u, \overline{z} - \mu \rangle^2 \leq O(\epsilon) \,.
    \]
    Put everything together to show that there is a polynomial time algorithm that takes the samples $v_1', \dots , v_n'$ and with probability $0.9$, outputs a $k$-sparse $\widehat{\mu}$ such that $\| \mu - \widehat{\mu} \| \leq O(\sqrt{\epsilon})$.
  \end{enumerate}

\end{problem}

\begin{problem}
  Recall the \emph{planted clique} problem, with the ``null distribution'' $\mathcal{N} = G(n,1/2)$, and the ``planted distribution'' $\mathcal{P}$ obtained by drawing $G$ from $G(n,1/2)$, and adding a uniformly random $k$-clique. It is believed that for $k$ significantly smaller than $O(\sqrt{n})$ (say $O(n^{1/2 - \eps})$), it is computationally hard to distinguish these two distributions. In this question, we will establish this computational hardness for the restricted class of algorithms based on low-degree polynomials.

  Concretely, set $k = O(n^{1/2 - \eps})$ for some (small) constant $\eps > 0$, and $D \le C \log n$ for some (large) constant $C > 0$. Recall the degree-$D$ $\chi^2$-divergence, defined by
  \[ \sqrt{ \chi^2_{\le D}\left( \mathcal{P} \| \mathcal{N} \right) } = \max_{\substack{F : \{ \text{set of graphs on $n$ vertices} \} \to \R \\ F \text{ degree $\le D$ polynomial} \\ F \text{ not identically $0$}}} \frac{ \E_{\mathcal{P}}[F] - \E_{\mathcal{N}}[F] }{\sqrt{\Var_{\mathcal{N}}[F]}}. \]
  Further recall that this maximum is attained by the function $\left( \frac{\mathcal{P}}{\mathcal{N}} \right)^{\le D}$, where $\frac{\mathcal{P}}{\mathcal{N}}$ is the likelihood ratio $\frac{\mathcal{P}}{\mathcal{N}}(G) = \frac{\mathcal{P}(G)}{\mathcal{N}(G)}$ and the notation $f^{\le D}$ denotes the projection of $f$ to the space of degree $D$ polynomials. This resulting maximum is equal to
  \[ \chi^2_{\le D} \left( \mathcal{P} \| \mathcal{N} \right) = \left\| \left( \frac{\mathcal{P}}{\mathcal{N}} \right)^{\le D} - 1 \right\|_2^2, \]
  with the notation $\|f\|_2^2 = \E_{\mathcal{N}} f^2$.

  \begin{enumerate}[label=(\alph*)]
    \item Let $g = \left( \frac{\mathcal{P}}{\mathcal{N}} \right)^{\le D}$ be a polynomial of degree $D$ in the variables $(x_e)_{e \in \binom{[n]}{2}}$, where $x_e = 1$ if $e$ is an edge in the graph, and $-1$ otherwise. Express $g$ in terms of its Fourier coefficients as $g = \sum_{\alpha : |\alpha| \le D} \wh{g}_{\alpha} x^\alpha$. Determine $\wh{g}_\alpha$.
    \item Show that in the given parameter regime of $k,D$, $\chi^2_{\le D}\left( \mathcal{P} \| \mathcal{N} \right) = \|g - 1\|^2 = o(1)$.
  \end{enumerate}
\end{problem}

\end{document}
